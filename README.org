#+TITLE: MCFA
#+SUBTITLE:  Mixtures of Common Factor Analyzers with missing data

This ~python~ package implements the _Mixtures of Common Factor Analyzers_ model
introduced by [[https://ieeexplore.ieee.org/document/5184847][Baek+ 2010.]] It uses [[https://www.tensorflow.org/][tensorflow]] to implement the stochastic
gradient descent, which allows for model training without prior imputation of
missing data. The interface resembles the [[https://scikit-learn.org/stable/][sklearn]] model API.

* Examples

** 3D-Gaussian distribution

#+html: <p align="center"><img src="gfx/complete_case_data_space.png" /></p>

** 3D-Gaussian distribution with missing data

* Install

To add the package to your ~python~ environment, clone the repository and run

#+begin_src shell
$ pip install --editable .
#+end_src

in the top-level directory.

* Usage

** Training

*** Model Parameters

The model contains six trainable parameters.

*** Parameter Initialization


*** Convergence of Loss function

A simple loss function convergence criterion is implemented: If the difference between the loss value averaged over the past ~converge_epochs~ (default is 10) and the current loss is within ~converge_perc~ (default is 1) percent, it is considered to have converged.

*** GPU vs CPU

If a GPU is available and properly set up, ~tensorflow~ will automatically execute the model training on it. To disable
this behaviour, add

#+begin_src python
import os
os.environ["CUDA_VISIBLE_DEVICES"] = "-1"
#+end_src

before the ~tensorflow~ import.

*** Validation Set

*** Training speed

~n_epochs~, ~batch_size~

Filled with all-zero rows. If your dataset contains all-zero rows, these are ignored during training.

** Clustering

~model.tau~, ~model.clusters~
model.compute_cluster_moments()
model.impute()

* Limitations

No unittests, Model parameter initialization requires some complete rows.

Limited initialzation of parameters.

* Other implementations of MCFA

- [[https://ieeexplore.ieee.org/document/5184847][Baek+ 2010]] [~R~]
- [[https://github.com/andycasey/mcfa][Casey+ 2019]] [~python~]. The difference to this implementation is the use of
  the EM-algorithm instead of gradient descent and the imputation of the missing
  values before the model training. Our implementation does not offer as many
  initialization routines for the lower space loadings and factors.
