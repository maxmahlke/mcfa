#+TITLE: MCFA
#+SUBTITLE:  Mixtures of Common Factor Analyzers with missing data

This ~python~ package implements the _Mixtures of Common Factor Analyzers_ model
introduced by [[https://ieeexplore.ieee.org/document/5184847][Baek+ 2010]]. It uses [[https://www.tensorflow.org/][tensorflow]] to implement the stochastic
gradient descent, which allows for model training without prior imputation of
missing data. The interface resembles the [[https://scikit-learn.org/stable/][sklearn]] model API.

* Examples

The two examples below illustrate the clustering in lower dimensional space using the example Gaussian clusters from [[https://ieeexplore.ieee.org/document/5184847][Baek+ 2010]], Section 5.

** 3D-Gaussian distribution

250 points drawn each from two 3D Gaussian distributions. The blue and red colours show the assigned cluster after applying MCFA. The black ellispe and x mark the cluster covariances and centers.

#+html: <p align="center"><img src="gfx/complete_case_data_space.png" width=700 /></p>

Illustration of the latent space and cluster properties.

#+html: <p align="center"><img src="gfx/complete_case_latent_space.png" width=700 /></p>

The loss function is used to evaluate the convergence of the SGD optimization.

#+html: <p align="center"><img src="gfx/complete_case_loss.png" width=700 /></p>

** 3D-Gaussian distribution with missing data

This example uses the same clusters as above, only that 20% of all observed features are randomly removed. The model is trained as before and later the missing data is imputed using the derived cluster properties and cluster associations. Datapoints with imputed features are drawn with 50% opacity below.

#+html: <p align="center"><img src="gfx/missing_case_data_space.png" width=700 /></p>

#+html: <p align="center"><img src="gfx/missing_case_latent_space.png" width=700 /></p>

* Install

To add the package to your ~python~ environment, clone the repository and run

#+begin_src shell
$ pip install --editable .
#+end_src

in the top-level directory.

* Usage

The MCFA is executed in three steps: initializing the model, training the parameters, and clustering the observations.

** Initialization

The model's main parameters are the number of clusters (~n_components~) and the number of latent factors (~n_factors~). These are set at when the model is instantiated.

#+begin_src python
from mcfa import MCFA

model = MCFA(n_components=2, n_factors=3)
#+end_src

** Training

Besides the observed data ~Y~, there are a handful of parameters which affect the model training.

#+begin_src python
model.fit(Y, n_epochs,  learning_rate, batch_size, frac_validation, converge_epochs)
#+end_src


*** Training speed

The number of times that the model is trained on the dataset is set by ~n_epochs~. The ~learning_rate~ parameter set the learning rate of the Adam optimizer used for the SGD. It's default value is [[https://twitter.com/karpathy/status/801621764144971776?lang=en][3e-4]]. The ~batch_size~ parameter is passed to the ~tensorflow.data.Dataset.batch()~ method and defines the size of the batches that the observations are grouped in. Default is ~32~.

*** Convergence of Loss function

The negative log-likelihood of the model is used as loss function. The ~frac_validation~ parameter (default is ~0.1~) defines the fraction of observation samples that is used as validation set.
The validation and training loss are accessible via the ~model.loss_validation~ and ~model.loss_training~ attributes after training, and can be plotted using the ~model.plot_loss()~ method.

A simple loss function convergence criterion is implemented: If the loss value averaged over the past ~converge_epochs~ epochs  (default is ~10~) is larger than the loss averaged over the equal number of epochs before that, the training is considered to have converged. It may not be reliable and you should always look at the loss function after training.

*** Parameter Initialization

The trainable model parameters are initialized using Principal Components Analysis of the complete-case data followed by clustering with Gaussian Mixture Models. Note that this requires to have complete observations in the dataset. If this is not the case, consider adding ~statsmodels.multivariate.PCA~ as initialization method, which supports Probabilistic PCA with missing data.

*** GPU vs CPU

If a GPU is available and properly set up, ~tensorflow~ will automatically execute the model training on it. To disable
this behaviour, add

#+begin_src python
import os
os.environ["CUDA_VISIBLE_DEVICES"] = "-1"
#+end_src

before the ~tensorflow~ import.

** Clustering

After training the model, any observations with the same features can be clustered via

#+begin_src python
model.transform(Y)  # if no data is passed, the training data is used
#+end_src

Note that ~Y~ does not necessarily have to be the same data that the model was trained on. The data may contain missing features.
After the ~transform()~ method, the assigned clusters are accessible via the ~model.clusters~ attribute. The entire responsibility matrix ~tau~ is available via the ~model.tau~ attribute.

** Optional: Imputation

The clustered data may be imputed by running

#+begin_src python
model.impute()
#+end_src

The imputed dataset is available via the ~model.Y_imp~ attribute.

* Limitations

- Model parameter initialization requires some complete rows.
- ~0~ is used as a marker for missing values in this implementation. If the observed features of an observation are all equal to zero, it is dropped.

* Other implementations of MCFA

- [[https://github.com/suren-rathnayake/EMMIXmfa][Baek+ 2010]] in ~R~
- [[https://github.com/andycasey/mcfa][Casey+ 2019]] in ~python~. The difference to this implementation is the use of
  the EM-algorithm instead of gradient descent and the imputation of the missing
  values before the model training. Further, our implementation does not offer as many
  initialization routines for the lower space loadings and factors.
